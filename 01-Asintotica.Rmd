---
output:
  html_document: default
  pdf_document: default
---


```{r message=FALSE, warning=FALSE, include=FALSE}
library(evd)
```


<!-- ecuaciones, su label y ref
\begin{equation} 
  f\left(k\right) = \binom{n}{k} p^k\left(1-p\right)^{n-k}
  (\#eq:binom)
\end{equation} 

You may refer to using `\@ref(eq:binom)`, like see Equation \@ref(eq:binom).
-->

# 

## La teoría asintótica clásica, las distribuciones extremales y sus dominios de atracción  {.unlisted .unnumbered}


Se dice que tenemos _datos extremos_ cuando cada
dato corresponde al máximo o mínimo de varios
registros. Ejemplos de este tipo de datos son:

- La máxima altura semanal de la ola en una
plataforma marina o portuaria $(m)$.
- La máxima velocidad de viento en determinada
dirección a lo largo de un mes $(km/h)$.
- La temperatura ambiental mínima a lo largo de
un día $(\dot{C})$.
- La temperatura ambiental mínima a lo largo de
un día ($\dot{C}$)
- La máxima velocidad de tráfico en un enlace de
una red de datos de datos en una hora ($Mb/s$).
- El mayor registro en un conteo de Coliformes
fecales sobre agua costeras al cabo de quince días.

Son un caso particular de evento raro o gran
desviación respecto a la media.
En resumen, en una gran variedad de dominios
disciplinares suele ser de gran interés el trabajo
con datos extremos, los que admiten diversos
enfoques. Entre ellos, los propios al párrafo
anterior (eventos raros, grandes desviaciones), que
se verán en el curso.
Sin embargo, el comienzo del curso se centra en la
teoría más clásica de estadística de datos extremos,
basada en el trabajo de Fréchet, Gumbel, Weibull,
Fisher, Tippett, Gnedenko, entre otros.

__Observación 1:__  Se recuerda que si $X$ e $Y$ son variables aleatorias independientes, cuyas
distribuciones son, respectivamente, $F$ y $G$,
entonces la variable

\begin{equation}
\max \left( X,Y \right)
(\#eq:1)
\end{equation}

tiene por distribución la función $H$ definida por 


\begin{equation}
H(t)= F(t)\; G(t)
(\#eq:2)
\end{equation}


__Observación 2:__  En esta parte inicial del curso
asumiremos que nuestros datos son $iid$
(independientes e idénticamente distribuidos, son
dos suposiciones juntas). Esta doble suposición
suele NO ser realista en aplicaciones concretas
(ninguna de sus dos componentes, incluso) pero
para comenzar a entender la teoría clásica, la
utilizaremos por un tiempo.


__Observación 3:__ Resulta claramente de la
Observación 1, que si tenemos datos $X_1,...,X_n$ $iid$ con distribución $F$, entonces

\begin{equation}
X_n^{\ast}= \max \left( X_1,...,X_n \right)
\end{equation}

tiene distribución $F_n^\ast$ dada por

\begin{equation}
F_n^\ast (t) = F(t)^n
\end{equation}

Si conocemos la distribución $F$ conoceríamos la
distribución $F_n^\ast$, pero en algunos casos la lectura
que queda registrada es la del dato máximo y no la
de cada observación que dio lugar al mismo, por lo
que a veces ni siquiera es viable estimar $F$.
Pero aún en los casos en que $F$ es conocida o
estimable, si $n$ es grande, la fórmula de $F_n^\ast$ puede resultar prácticamente inmanejable. En una línea de trabajo similar a la que aporta el *Teorema
Central del Límite* en la estadística de valores
medios, un teorema nos va a permitir aproximar
$F_n^\ast$ por distribuciones más sencillas. Este es el
*Teorema de Fischer-Tippet-Gnedenko* (FTG) que presentaremos en breve.

__Observación 4:__ Si $X_1,...,X_n\;$ es $iid\;$ y definimos
$\;Y_i = -X_i\;$ para todo valor de $i$, entonces $Y_1,...,Y_n\;$ es $iid\;$ y además

\begin{equation}
min(X_1,...,X_n) = - max(Y_1,...,Y_n)
\end{equation}

la teoría asintótica de los mínimos de datos $iid$
se reduce a la de los máximos, razón por la que
nos concentramos aquí en estudiar el
comportamiento asintótico de los **máximos**
exclusivamente. 

__Definición 1: Las distribuciones extremales.__

Las distribuciones extremales son tres: la
*distribución de Gumbel*, la *distribución de Weibull* y
la *distribución de Fréchet*. En su versión *standard* o *típica* se definen del modo
siguiente.

Se dice que una variable tiene distribución de:

-__Gumbel__ si su distribución es
 
$$\Lambda(x) = e^{\{-e^{-x}\}}\hspace{0.3cm}\text{ para todo }\: x \;\text{real}.$$


-__Weibull__ de orden $\alpha>0$ si su distribución es


$$\Psi_{\alpha}(x)=\begin{cases}
e^{\left\{-(-x)^{\alpha}  \right\}} & si\;x<0\\
1 & \text{en otro caso}
\end{cases}$$


-__Fréchet__ de orden $\alpha>0$ si su distribución es

$$
\Phi_{\alpha}(x)=\begin{cases}
e^{\left\{ -x^{-\alpha}\right\}} & si\;x>0\\
0 & \text{en otro caso}
\end{cases}
$$
__Nota:__ Como los máximos en general son valores grandes,
importa particularmente observar el comportamiento de estas distribuciones para $x$ tendiendo a infinito. El límite es $1$ como en toda distribución. Pero *VA MAS RAPIDO* a 1 la Weibull, luego la Gumbel y luego la Fréchet. Esto es indicio que la
Fréchet modela datos *más extremos*, máximos de datos de
colas más pesadas que la Gumbel y ésta que la Weibull. Más
adelante veremos esto más precisamente. En la Fréchet, la
velocidad de convergencia a 1 crece al aumentar el orden. En cambio en la Weibull el orden afecta la velocidad con que va a 0 cuando $x$ tiende a menos infinito, que crece cuanto mayor el orden. Esto quedará más claro con el Teorema 1 del curso. La visualización de las densidades de cada tipo quizás ayude a comprender mejor los pesos relativos de las colas.


```{r plot-extreme-distributions, echo=FALSE, fig.height=5, fig.width=6, message=FALSE, warning=FALSE}
# Load necessary package
library(ggplot2)

# Define the range of x values, ensuring negative values are included
x_gumbel <- seq(-5, 5, length.out = 1000)
x_weibull <- seq(-5, 5, length.out = 1000)  
x_frechet <- seq(-5, 5, length.out = 1000)  # Extended to include negative values

# Define the PDFs based on given formulas
gumbel_pdf <- function(x) exp(-x) * exp(-exp(-x))
weibull_pdf <- function(x, alpha = 1) ifelse(x < 0, alpha * (-x)^(alpha - 1) * exp(-(-x)^alpha), 0)
frechet_pdf <- function(x, alpha = 1) ifelse(x > 0, alpha * x^(-alpha - 1) * exp(-x^(-alpha)), 0)  # Explicitly defined for x ≤ 0

# Compute PDFs
gumbel_vals <- gumbel_pdf(x_gumbel)
weibull_vals <- weibull_pdf(x_weibull, alpha = 1)  # Order 1
frechet_vals <- frechet_pdf(x_frechet, alpha = 1)  # Order 1

# Create data frames for ggplot
df_gumbel <- data.frame(x = x_gumbel, y = gumbel_vals, Distribution = "Gumbel")
df_weibull <- data.frame(x = x_weibull, y = weibull_vals, Distribution = "Weibull (α=1)")
df_frechet <- data.frame(x = x_frechet, y = frechet_vals, Distribution = "Fréchet (α=1)")

# Combine all data
df <- rbind(df_gumbel, df_weibull, df_frechet)

# Plot using ggplot2
ggplot(df, aes(x, y, color = Distribution)) +
  geom_line(size = 1) +
  labs(title = "Densidades Extremales",
       x = "x", y = "Densidad") +
  theme_minimal() +
  scale_color_manual(values = c("blue", "red", "green"))
```


A estas versiones standard se las puede extender
agregando un parámetro de recentramiento $(\mu)$ y
un parámetro de escala $(\beta)$. 

Se dice que $X$ tiene distribución: 

- __Gumbel__ : $\Lambda^{(\mu, \beta)}$ si $\;X=\mu + \beta Y\;$, donde $Y$ tiene distribución $\Lambda$.
- __Weibull__: $\;\Psi^{(\mu, \beta)}\;$ si $\;X=\mu + \beta Y\;$, donde $Y$ tiene distribución $\Psi_{\alpha}$.

- __Fréchet__: $\;\Phi^{(\mu, \beta)}\;$ si $X=\mu + \beta Y$, donde $Y$ tiene distribución $\Phi_{\alpha}$.


En general, es en este sentido que diremos que una
variable es Gumbel, Weibull o Fréchet (incluyendo
recentramiento y reescalamiento), pero en cálculos
donde los parámetros $\mu$ y $\beta$ no sean relevantes, por
simplicidad, usaremos las versiones standard. 


El siguiente teorema vincula las distribuciones
extremales en sus formatos standard y resulta de
gran utilidad práctica sobre todo al hacer tests de
ajustes, etc.

__Teorema 1 : *Relaciones entre las versiones
standard de las distribuciones extremales.*__


$X$ tiene distribución $\Phi_{\alpha}$ $\Leftrightarrow$  $(-1/X)$ tiene distribución $\Psi_{\alpha}$ $\Leftrightarrow$ $\log(X^{\alpha})$ tiene distribución $\Lambda$. 


Nota: en otros contextos de la Estadística (en particular en
alguna rutinas del R), se le llama Weibull a una variable que
corresponde a -X, con X Weibull como definimos nosotros.

__Observación 5:__  Recordamos que la función
Gamma ($\Gamma$ ), que extiende a la función factorial
($\Gamma(n)=n-1!\quad \forall n$ natural) definida por


\begin{equation}
\Gamma(x)=\int_{0}^{\infty} t^{u-1}e^{-t}dt
\end{equation}

es una función disponible tanto en el software R
como en planillas de cálculo, etc.


__Teorema 2: (Tres en uno) *Algunos datos de las
distribuciones extremales.*__

__Parte 1__

Si $X$ tiene distribución $\Lambda^{(\mu,\beta)}$ entonces tiene:



a) __Esperanza__: $E(X) = \mu + \beta\gamma$, donde $\gamma$ es la constante de Euler-Mascheroni, cuyo valor aproximado es $0.5772156649$.

b) __Moda__:  $\text{moda}(X)=\mu$

c)  __Mediana__: $\text{med}(X)=\mu - \beta \log(\log 2) \approx \mu - 0.36651 \beta$

d)  __Desviación estándar__: $\sigma(X)=\frac{\beta \pi}{\sqrt{6}}   \approx 1.2825 \beta$

e) Si $X^+ = \max(X,0)$, entonces $E(X+k)$ es finito para todo valor de $k$ natural

f) Para simular computacionalmente $X$, se puede tomar $U$ uniforme en $(0,1)$ y hacer $X = \mu - \beta \log(-\log U)$.


__Parte 2__

Si $X$ tiene distribución $\Psi^{(\mu, \beta)}$ entonces tiene:

a) $E(X)=\mu -\beta \Gamma (1+1/\alpha)$

b) \begin{equation*}\text{moda}(X) =\begin{cases} 
  \mu  & \text{si }\; \alpha \leq 1 \\
    \mu-\beta\left\{ \frac{\left( \alpha-1 \right)}{\alpha} \right\}^{1/\alpha} & \text{si }\; \alpha >1
\end{cases}\end{equation*}


c) $\text{med}(X)=\mu - \beta (\log 2)^{\frac{1}{\alpha}}$

d) $\sigma(X)=\beta\left\{\Gamma\left( 1+\frac{2}{\alpha} \right)-\Gamma\left( 1+\frac{1}{\alpha} \right)^2  \right\}^{1/2}$.

__Parte 3__

Si $X$ tiene distribución $\Phi_{\alpha}^{(\mu, \beta)}$ entonces tiene:

a) 
\begin{equation*}
E(x) =
\begin{cases} 
    \mu + \beta\;\Gamma\left( 1-\frac{1}{\alpha} \right) & \text{si } \alpha>1 \\
    \infty & \text{en otro caso}
\end{cases}
\end{equation*}

b)  $\text{moda}(X)=\mu+ \beta\;\left\{ \frac{\alpha}{\left( 1+ \alpha\right)}\right\}^{1/\alpha}$

c) $\text{med}(X)=\mu + \beta \;\left( \log 2 \right)^{\left( -1/\alpha \right)}$

d) \begin{equation*}
\sigma(x) =
\begin{cases} 
    \mu + \left| \Gamma \left( 1 - \frac{2}{\alpha} \right) - \Gamma \left(  1 - \frac{1}{\alpha}\right)\right|  & \text{si } \; \alpha>2 \\
    \infty & \text{si } \; 1<\alpha \leq 2
\end{cases}
\end{equation*}


__Observación 6:__ El item e) de la Parte 1 es
trivialmente cierto para Weibull y tomando en
cuenta el item a) de la Parte 3, es claramente falso
para Fréchet.

__Observación 7:__ El item f) de la Parte 1 en
conjunto con el Teorema 1 brinda fórmulas
sencillas para simular computacionalmente
distribuciones Weibull o Fréchet.

__Observación 8:__ Se generaron mil números aleatorios y aplicando el
item f) de la Parte 1: se simularon mil variables
Gumbel standard $iid$, calculándose su promedio, su
desviación standard empírica y su mediana
empírica. 

```{r echo=TRUE}
# Fijar semilla para reproducibilidad
set.seed(123)

# Definir parámetros
mu <- 0       # Centro
beta <- 1     # Escala
gamma <- 0.5772156649  # Constante de Euler-Mascheroni

# Número de simulaciones
n <- 1000

# Generar 1000 valores de una variable uniforme en (0,1)
U <- runif(n)

# Simular la variable Gumbel con parámetros (mu, beta)
X_gumbel <- mu - beta * log(-log(U))

# Calcular estadísticas
esperanza <- mu + beta * gamma
moda <- mu
mediana_teorica <- mu - beta * log(log(2))
desviacion_std_teorica <- beta * pi / sqrt(6)

# Calcular estadísticas empíricas
promedio_empirico <- mean(X_gumbel)
desviacion_std_empirica <- sd(X_gumbel)
mediana_empirica <- median(X_gumbel)
```

Los resultados fueron los siguientes:

```{r echo=FALSE}
# Mostrar resultados teóricos
cat("----- Resultados teóricos: ----- \n")
cat("Esperanza teórica:", esperanza, "\n")
cat("Moda teórica:", moda, "\n")
cat("Mediana teórica:", mediana_teorica, "\n")
cat("Desviación estándar teórica:", desviacion_std_teorica, "\n\n")

# Mostrar resultados empíricos
cat("----- Resultados empíricos (simulación con n =", n, "): -----\n")
cat("Promedio empírico:", promedio_empirico, "\n")
cat("Desviación estándar empírica:", desviacion_std_empirica, "\n")
cat("Mediana empírica:", mediana_empirica, "\n")
```

Observar que los resultados empíricos están cerca del valor esperado, desvío standard y mediana de la Gumbel standard.


A continuación presentaremos el Teorema medular de esta primera parte, expresado de la manera más llana posible. Veremos posteriormente algunos detalles con más cuidado. En particular, veremos que la continuidad de la distribución $F$ no
es una hipótesis real (ni es necesaria ni es suficiente, por eso la
entrecomillamos), pero ayuda a visualizar que no vale el teorema para toda distribución $F$, así como veremos con cierto detalle más adelante...


__Teorema 3: de Fischer-Tippet-Gnedenko (FTG)__ 

Si $X_1,...,X_n$ es $iid$ con distribución $F$ 'continua',
llamamos $F^{\ast}_n$ a la distribución de $max(X_1,...,X_n)$ y $n$
es grande, entonces existen $\mu$ real y $\beta > 0$ tales que
alguna de las siguientes tres afirmaciones es
correcta:

a) $F^{\ast}_n$ se puede aproximar por la distribución
de $\mu+\beta Y$, con $Y$ variable con distribución $\Lambda$.
b) Existe $\alpha>0$ tal que $F_n^{\ast}$ se puede aproximar por la distribución de $\mu+\beta Y$ con $Y$ variable con distribución $\Phi_{\alpha}$. 
c) Existe $\alpha>0$ tal que $F_n^{\ast}$ se puede aproximar por la distribución de $\mu+\beta Y$ con $Y$ variable con distribución $\Phi_{\alpha}$. 


Lo anterior equivale a decir que la distribución del máximo de datos _continuos_ e $iid$, si $n$ es grande, puede aproximarse por una Gumbel, una Fréchet o una Weibull. 

__Observación 9:__ Como veremos con cierto detalle, cuál de las tres aproximaciones es la válida depende de cómo sea la distribución $F$.

Por ejemplo, veremos que:

- Si $F$ es normal o exponencial, se aplica a $F_n^{\ast}$ la aproximación por una Gumbel .
- Si $F$ es uniforme, vale para $F_n^{\ast}$ la aproximación por una Weibull.
- Si $F$ es Cauchy, la aproximación válida para $F_n^{\ast}$ es por una Fréchet.

Más precisamente, cuál de las tres aproximaciones es la aplicable depende de la cola de $F$^[Los valores de $F(t)$ para valores grandes de $t$.].

En concreto, Weibull aparece cuando $F$ es la
distribución de una variable acotada por arriba
(como la Uniforme), Gumbel para distribuciones
de variables no acotadas por arriba pero con colas muy livianas (caso Exponencial y Normal) y Fréchet para colas pesadas (caso Cauchy).
Finalmente, si bien aclaramos que la hipótesis de continuidad de $F$ no es esencial, veremos que si $F$ es la distribución Binomial o Poisson, por mencionar
dos ejemplos muy conocidos y sencillos, NO se
puede aplicar ninguna de las tres aproximaciones
anteriores.


__Observación 10.__ Como consecuencia del $FTG$ si se tienen datos de máximos, las distribuciones extremales son “candidatas” razonables para proponer en un ajuste.
Sin embargo no debe pensarse que siempre se va a lograr ajustar a una de las tres distribuciones extremales, ya que hay al menos dos causas evidentes que podrían desbaratar la aplicación del FTG:

1) Que la cantidad de registros que se consideran al
calcular cada máximo no sea suficientemente
grande. 

2) Que los registros que se consideran al calcular cada máximo no sean $iid$^[Al final del capítulo 2 se verá que esto puede subsanarse con versiones más generales del FTG.].

Por consiguiente el $FTG$ alienta a intentar ajustar datos
extremales a una de las tres distribuciones extremales, pero no
siempre un tal ajuste dará un resultado afirmativo.

__Ejemplo 1.__ Veamos un ejemplo de ajuste. Los
siguientes datos corresponden a los valores, en $80$ puntos geográficos distintos de la región parisina, del máximo estival del contaminante atmosférico $O_3$ (no perceptible sensorialmente y con impacto
sanitario serio). Cada dato es el máximo registro en cada sensor a lo largo de todo un verano; el contaminante se mide diariamente, por lo cual, cada uno de nuestros $80$ datos es el máximo de unas $100$ lecturas diarias.

```{r echo=FALSE, paged.print=TRUE}
print("Primeros 6 datos:")
data <- data.frame(
  i = 1:80,
  X_i = c(430.3, 115.7, 4.48, 26.95, 72.27, 206.4, 22.79, 25.03, 226.8, 11.1,
        1572, 100, 104.5, 37.1, 20.22, 106.9, 47.2, 62.82, 39.3, 18.52,
        41.47, 429.5, 1228, 127.6, 9.93, 90.4, 201.7, 295.1, 20.62, 20.58,
        13.27, 538.1, 804, 321.6, 16.11, 22.05, 100.2, 40.76, 262.7, 19.32,
        7.79, 58.02, 28.02, 18.38, 13.12, 572.8, 44.46, 40.72, 25.07, 24.07,
        511.8, 38.12, 15.86, 75.48, 24.09, 119.4, 174.7, 104.7, 140, 79.67,
        158, 25.46, 462.5, 35.53, 876.4, 462.5, 53.47, 23.59, 38.77, 494.2,
        164.2, 52.06, 54.13, 15.53, 29, 14.35, 1675, 15.01, 72.07, 22.99))
```

```{r echo=FALSE, paged.print=FALSE}
head(data)
```

# Convertir a DataFrame
df = pd.DataFrame(data)

<!--

Siguiendo a @notas_curso
 se dice que tenemos datos extremos cuando cada
dato corresponde al máximo o mínimo de varios
registros. Son un caso particular de evento raro o gran 
desviación respecto a la media.

Asumiremos que nuestros datos son $iid$
(independientes e idénticamente distribuidos, son
dos suposiciones juntas). Esta doble suposición
suele no ser realista en aplicaciones concretas
(ninguna de sus dos componentes, incluso) pero
para comenzar a entender la teoría clásica, la
utilizaremos por un tiempo.

Si tenemos datos $X_1,...,X_n$ $iid$
con distribución $F$, entonces $X_n^* = max (X_1,...,X_n)$ tiene distribución $F_n^*$ dada por
$F_n^* (t)= F(t)_n$. Si conocemos la distribución $F$ conoceríamos la 
distribución $F_n^*$
, pero en algunos casos la lectura 
que queda registrada es la del dato máximo y no la 
de cada observación que dio lugar al mismo, por lo 
que a veces ni siquiera es viable estimar $F$.
Pero aún en los casos en que $F$ es conocida o 
estimable, si $n$ es grande, la fórmula de $F_n^*$ puede 
resultar prácticamente inmanejable. En una línea de trabajo similar a la que aporta el Teorema 
Central del Límite en la estadística de valores 
medios, un teorema nos va a permitir aproximar 
$F_n^*$ por distribuciones más sencillas. Este es el 
Teorema de Fischer-Tippet-Gnedenko (FTG, para 
abreviar) que presentaremos en breve.


Como $X_1,...,X_n$ iid, definimos 
$Y_i = -X_i$ para todo valor de $i$, entonces $Y_1,...,Y_n$ iid y
además
$min(X_1,...,X_n) = - max(Y_1,...,Y_n)$
la teoría asintótica de los mínimos de datos iid
se reduce a la de los máximos, razón por la que 
nos concentramos aquí en estudiar el 
comportamiento asintótico de los máximos 
exclusivamente.

\newpage

### Definición 1: Las distribuciones extremales

Las distribuciones extremales son tres: la
distribución de Gumbel; la distribución de Weibull; 
la distribución de Fréchet.

#### Distribución de Gumbel




Se dice que una variable tiene distribución de 
Gumbel si su distribución es: 

$$ \Lambda(x) = exp\{-e^{-x}\} \quad\text{para todo}\; x \;\text{real} $$

Cuando tomamos los máximos de variables no acotadas pero que tienen colas livianas (ej. la distribución tiene probabilidades muy bajas de tomar valores lejos de la media) los mismos convergen a una distribución asintótica extremal de Gumbel.

Para simular distribuciones de Gumbel, utilizamos el paquete __evd__ de @evd y en particular la función __pgumbel__. Partiendo de una simulación de números aleatorios, para un secuencia de 1000 números entre $[-10,10]$, se tienen las siguientes figuras \@ref(fig:gumbel_plots)  relativas a la CDF y PDF de la distribución Gumbel. 
 




```{r gumbel_plots, fig.cap="CDF and PDF for Gumbel distribution.", echo=FALSE, ,echo=FALSE}
# Define the sequence of x values
x_aux <- seq(-10, 10, length = 1000)

# Set up the plotting layout
par(mfrow = c(3, 1), mar = c(5, 4, 3, 1))

# Plot the cumulative distribution function (CDF) of the Gumbel distribution
plot(seq(-10, 10, length = 100), pgumbel(q = seq(-10, 10, length = 100), loc = 0, scale = 1), 
     xlim = c(-10, 10), type = "l", ylab = "F(x)", xlab = "x", main = "Cumulative Distribution Function (CDF)")
lines(seq(-10, 10, length = 100), pgumbel(q = seq(-10, 10, length = 100), loc = 0, scale = 2), col = "red")
title(main ="", sub = "Original Gumbel")

# Plot the probability density function (PDF) of the Gumbel distribution
plot(x_aux, dgumbel(x = x_aux, loc = 0, scale = 1, log = FALSE), xlim = c(-10, 10), type = "l", 
     ylab = "f(x)", xlab = "x", main = "Probability Density Function (PDF)")
lines(x_aux, dgumbel(x = x_aux, loc = 0, scale = 2, log = FALSE), col = "red")
title(main = "")

# Add subtitles to each plot
title(main = "", sub = "Rescaled Gumbel", col.main = "red", col.sub = "red")
```



Si calculamos el valor esperado y el desvío estandard de estos valores observados y tenemos una muestra lo suficientemente grande, podremos comparar los resultados con los esperados de forma teórica.

```{r}
# Podemos simular 100 datos aleatorios de una distribución Gumbel
GumbelAleatorio<-rgumbel(100)
plot(density(GumbelAleatorio))
```
```{r}
-digamma(1) # Constante de Euler-Mascheroni
```

```{r}
mean(rgumbel(1000))
```

```{r}
sd(rgumbel(1000))
```


#### Distribución de Weibull

Se dice que una variable tiene distribución de 
Weibull de orden $\alpha>0$ si su distribución es:

$$\Psi_{\alpha}(x)=\begin{cases}
exp{-(-x)^{\alpha}} & si\;x<0\\
1 & \text{en otro caso}
\end{cases}$$
Recordemos que cuando tomamos los máximos de las variables $iid$ con un rango acotado, la distribución resultante por la cual se puede aproximar es la de Weibull. En este caso, y en el resto del LAB, exp() y e son la función exponencial. 

Por una única vez, calculemos la distribución de forma “manual” en el R para convencernos de la forma de la función de distribución de Weibull ($\Psi$). Para eso generaremos un vector auxiliar de valores $x$ y la distribución ($F(x)$). En R la definición de la distribución es sutilmente diferente a la que vimos en el teórico (definida para positivos), pero totalmente convertible con dos cambios de signo. La función que calcula la probabilidad de una distribución Weibull es __pweibull()__. Pueden ver la definición de R utilizando help(pweibull) o ?pweibull.En R podemos saber la forma y valores de esta distribución con una función implementada en un paquete base {stats}. La función es pweibull y lleva como argumentos un vector de cuantiles ( q ), un argumento de forma ( shape ) y otro de escala ( scale ). Recordemos que la función plot utiliza 2 argumentos centrales ( x e y ) y podemos fijar los límites del gráfico ( xlim e ylim), el tipo de gráfico ( type) y las etiquetas de los ejes X e Y ( xlab e ylab).

Primero generaremos un vector de numeros auxiliares equiespaciados y lo nombraremos (“x_aux”). Luego definiremos un orden (alpha=α
) de la Weibull y graficaremos la función.

```{r echo=FALSE}
x_aux<-seq(-10, 10, length=100)
alpha<- 2.6 # definimos un alfa cualquiera (>0)
plot(x_aux, exp(-(-x_aux)^alpha), type="l", log="", ylab="F(X)", xlab="X", main="Distribucion de Weibull") # recordemos que la definimos para X<0, luego vale 1
points(x_aux, 1-pweibull(-x_aux, shape=alpha, scale=1), col="red")
abline(v=0)
```
Veamos ahora la forma de un par de distribuciones cambiando el parámetro de orden (α
), que en la función pweibull de R se nombra como shape y que define el orden de la distribución.

```{r echo=FALSE}
#Veamos ahora la forma de un par de distribuciones cambiando el parámetro de orden (α
#), que en la función pweibull de R se nombra como shape y que define el orden de la distribución.
x_aux<- seq(-10,10, length=1000)
par(mfrow=c(3,1), mar=c(5,4,3,1))
plot(x_aux, 1-pweibull(q=-x_aux, shape=2, scale=1) ,xlim=c(-5,5), type="l", ylab="F(x)", xlab="x", main="Weibull")
lines(x_aux, 1-pweibull(q=-x_aux, shape=1.1, scale=1), col="red")

plot(x_aux, dweibull(x=-x_aux, shape=2, scale=1, log = FALSE) ,xlim=c(-5,5), type="l", ylab="f(x)", xlab="x")
lines(x_aux, dweibull(x=-x_aux, shape=1.1, scale=1, log = FALSE), col="red")
```
En R podemos también generar numeros aleatórios (técnicamente pseudo-aleatorios) de una distribución extremal. Estos simuladores de números aleatórios son útiles para comparar contra distribuciones nulas, generar modelos sintéticos para probar algorítmos, etc…
Para lxs que venimos de la rama mas aplicada, muchas veces nos ayudan a entender como funcionan los modelos y a verificar si nuestra intuición es acertada respecto a la escala de ajuste de los parámetros entre otras útiles. Generaremos 2 series de 1000 números aleatórios con la función rweibull, que tiene como parámetro el número de datos que se necesitan y la forma (shape) de la distribución. Luego haremos un grafico con la densidad empírica (esto es similar a un histograma) de estos vectores.

```{r echo=FALSE}
#
randomWeibull1<-rweibull(1000, shape=2)
randomWeibull2<-rweibull(1000, shape=1.1)

plot(density(randomWeibull1), main="Weibul de una muestra aleatoria")
lines(density(randomWeibull2),col="red")
```


#### Distribución de Fréchet

Se dice que una variable tiene distribución de 
Fréchet de orden $\alpha>0$ si su distribución es:

$$
\Phi_{\alpha}(x)=\begin{cases}
exp\{-x^{-\alpha}\} & si\;x>0\\
0 & \text{en otro caso}
\end{cases}
$$


Esta tercera clase de variables incluyen a las distribuciones no acotadas, pero de colas pesadas. Es decir que tienen una probabilidad alta de presentar valores alejados de la media o la mediana (ej. la Cauchy). En estos casos, la distribución de sus máximos es la Frechet. Grafiquemos esta distribución para dos valores diferentes de $\alpha$.

```{r}
x_aux<- seq(-10,10, length=1000)

par(mfrow=c(3,1), mar=c(5,4,3,1))
plot(seq(-10,10,length=100), pfrechet(q=seq(-10,10,length=100), shape=2, scale=1) ,xlim=c(-2,10), type="l", ylab="F(x)", xlab="x", main="Frechet")
lines(seq(-10,10,length=100), pfrechet(q=seq(-10,10,length=100), shape=1.1, scale=1),col= "red")

plot(x_aux, dfrechet(x=x_aux, shape=2, scale=1, log = FALSE) ,xlim=c(-2,10), type="l", ylab="f(x)", xlab="x")
lines(x_aux, dfrechet(x=x_aux, shape=1.1, scale=1, log = FALSE), col="red")
```

.




\newpage

##### Teorema 1: Relaciones entre las versiones standard de las distribuciones extremales

$X$ tiene distribución $\Phi_{\alpha}(x)$ si y sólo si $(-1/X)$ tiene 
distribución $\Psi_{\alpha}(x)$ si y sólo si $log(X^{\alpha})$ tiene 
distribución $\Lambda$.


##### Teorema 2: Algunos datos de las distribuciones extremales
##### Parte 1
Si $X$ tiene distribución $\Lambda^{(\mu,\beta)}$ entonces tiene:

\begin{itemize}
  \item[a)] Valor esperado: $E(X) = \mu + \beta\gamma$, donde $\gamma$ es la constante de Euler-Mascheroni, cuyo valor aproximado es $0.5772156649$.
  \item[b)] Moda: $\mu$
  \item[c)] Mediana: $\mu - \beta \log(\log 2) \approx \mu - 0.36651 \beta$.
  \item[d)] Desviación estándar: $\beta \pi \sqrt{6} \approx 1.2825 \beta$.
  \item[e)] Si $X^+ = \max(X,0)$, entonces $E(X+k)$ es finito para todo valor de $k$ natural.
  \item[f)] Para simular computacionalmente $X$, se puede tomar $U$ uniforme en $(0,1)$ y hacer $X = \mu - \beta \log(-\log U)$.
\end{itemize}

#### Parte 2

Si $X$ tiene distribución $\Psi_{\alpha}^{(\mu,\beta)}$ entonces tiene: 

\begin{itemize}
  \item[a)] Valor esperado: $E(X) = \mu + \beta\Gamma(1+1/\alpha)$.
  \item[b)] Moda: $\mu$ si $\alpha\leq 1$ y $\mu-\beta\{(\alpha-1)/\alpha\}^{(1/\alpha)}$ si $\alpha>1$.
  \item[c)] Mediana: $\mu - \beta \log(2)^{(1/\alpha)}$.
  \item[d)] Desviación estándar: $\beta\{\Gamma(1+2/\alpha)-\Gamma(1+1/\alpha)^2\}^{1/2}$.
\end{itemize}

#### Parte 2

Si $X$ tiene una distribución $\Phi_{\alpha}^{(\mu, \beta)}$ entonces se tiene:

\begin{itemize}
  \item[a)] Valor esperado: $E(X) = \mu + \beta\Gamma(1-1/\alpha)$ si $\alpha > 1$, $\infty$ en caso contrario.
  \item[b)] Moda: $\mu + \beta\Gamma(1-1/\alpha)$ si $\alpha>1$.
  \item[c)] Mediana: $\mu + \beta \log(2)^{(-1/\alpha)}$.
  \item[d)] Desviación estándar: $\beta|\Gamma(1-2/\alpha)-\Gamma(1-1/\alpha)^2|$ si $\alpha>2$, $\infty$ si $1<\alpha \leq 2$.
\end{itemize}


\newpage

##### Teorema 3: Fischer-Tippet-Gnedenko (FTG)


Si $X_1,...,X_n\quad iid$ con distribución $F$ "continua", llamamos $F_n^*$ a la distribución de $max(X_1,...,X_n)$ y $n$ es grande, entonces existen $\mu$ real y $\beta>0$ tales que alguna de las siguientes tres afirmaciones es correcta:

\begin{itemize}
  \item[1)] $F_n^*$ se puede apromixar por la distribución de $\mu+\beta Y$ con $Y$ variable con distribución $\Lambda$.
  \item[2)] Existe $\alpha>0$ tal que $F_n^*$ se puede aproximar por la distribución de $\mu+\beta Y$ con $Y$ variable con distribución $\Phi_{\alpha}$. 
  \item[3)] Existe $\alpha>0$ tal que $F_n^*$ se puede aproximar por la distribución de $\mu+\beta Y$ con $Y$ variable con distribución $\Phi_{\alpha}$.
\end{itemize}

Lo anterior equivale a decir que la distribución del máximo de datos \textit{continuos} e $iid$, si $n$ es grande, puede aproximarse por una Gumbel, una Fréchet o una Weibull. Una aproximación será válida dependiendo de la distribución de $F$. En este sentido, cuando $F$ sea normal entonces $F_n^*$ se puede aproximar como una Gumbel. Cuando $F$ sea uniforme, se puede aproximar $F_n^*$ como una Weibull y cuando $F$ sea Cauchy entonces $F_n^*$ se puede aproximar por una Fréchet. 

Más precisamente, cuál de las tres aproximaciones es la aplicable depende de la cola de $F$ (los valores de $F(t)$ para valores grandes de $t$).
En concreto, Weibull aparece cuando $F$ es la distribución de una variable acotada por arriba (como la Uniforme), Gumbel para distribuciones de variables no acotadas por arriba pero con colas muy livianas (caso Exponencial y Normal) y Fréchet para colas pesadas (caso Cauchy)\footnote{Si bien  la hipótesis de continuidad de $F$ no es esencial, si $F$ tiene
la distribución Binomial o Poisson, por ejemplo, no se puede aplicar ninguna de las tres aproximaciones anteriores.}.

Como consecuencia del $FTG$ cuando se tengan datos máximos, las distribuciones maximales podrían ser candidatas de uno de los ajustes si

\begin{itemize}
\item la cantidad de registros es lo suficientemente grande
\item los registros son $iid$ aunque con versiones más generales del $FTG$ este supuesto puede no cumplirse
\end{itemize}

Como la mayoría de tests de ajustes suponen datos $iid$, se van a realizar dos tests de aleatoriedad\footnote{En inglés se expresa como \textit{randomness}} a los datos:

\begin{itemize}
\item  Runs up and down 
\item  Spearman correlation of ranks 
\end{itemize}

Se emplea la prueba de ajuste $\chi^2$ que requiere seleccionar una partición más o menos arbitraria de la recta real de intervalos siendo importante que en cada intervalo haya una cantidad lo suficientemente importante de datos de la muestra. En este sentido, se pueden tomar como extremos de los intervalos los quintiles empíricos de la muestra. Cabe mencionar que este test requiere estimar parámetros por el método de Máxia Verosimilitud Categórica.

Cabe mencionar que para este estudio la distribución de la variable a incorporar en este estudio no tiene que ser degenerada, es decir $H(t)=0$ ó $H(t)=1$.

\newpage

### Definición 2: Distribución extremal asintótica

Si $X_1,...,X_n$  es $iid$ con distribución $F$ diremos que $H$ no-degenerada es la Distribución Extremal Asintótica (DEA) de $F$\footnote{Lo que equivale a decir que $F$ tiene $DEA\;H$.}, si existen dos sucesiones de números reales, $d_n$ y $c_n>0$, tales que la distribución de

\begin{equation}
\frac{max(X_1,...,X_n)- d_n}{c_n}\label{eq:max}
\end{equation}

tiende a $H$ cuando $n$ tiende a infinito.


### Definición 3: Supremo esencial de una variable aleatoria o distribución

Si $X$ tiene distribución $F$, se llama supremo esencial de $X$, denotado como $M_X$ o, indistintamente, supremo esencial de $F$, denotado $MF$ a

\begin{equation}
M_X=M_F= sup\{t / F(t)<1\}\label{eq:Mx}
\end{equation}

Observación:
\begin{itemize}
\item Si $F$ es $U(a,b)$, $M_F=b$
\item Si $F$ es $Bin(m,p)$, $M_F=m$
\item Si $F$ es Normal, Exponencial, Cauchy o Poisson, $M_F$ es infinito.
\end{itemize}

##### Teorema 4

Si $X_1,...,X_n$ es $iid$ con distribución $F$ cualquiera, entonces, para $n$ tendiendo a infinito,

\begin{equation}
X^*_n=M_F= max(X_1,...,X_n)\;tiende\;a\;M_F\label{eq:Xast}
\end{equation}

Observación:

El resultado anterior vale incluso si $M_F$ es infinito, pero si $M_F$ es finito, como $X^*n - M_F$ tiende a cero, por analogía con el Teorema Central del Límite para promedios, buscaríamos una sucesión $c_n>0$ y que tienda a cero de modo tal que $(X^*n- M_F )/ c_n$ tienda a una distribución no-degenerada y de allí surge buscar la DEA.


##### Teorema 5


Si $F$ es una distribución con $M_F$ finito, y para $X$ con distribución $F$ se cumple que

$$
P(X=M_F)>0 
$$

entonces $F$ NO admite DEA.

Observación:

Si $F$ es $Bin(m,p)$, $M_F=m$. Si $X$ tiene distribución $F$, entonces
$P( X=M_F)= P( X=m)= p_m>0$,
asi que la distribucion $Bin(m,p)$ NO admite DEA, no se puede aproximar la distribución del máximo de una muestra $iid$ de variables $Bin(m,p)$.

El Teorema anterior es un caso particular del próximo.


##### Teorema 6

Si $F$ es una distribución con $M_F$ finito o infinito que admite DEA, y $X$ tiene distribución $F$, entonces el límite cuando $t$ tiende a $M_F$ por izquierda de
$P(X>t)/P(X \geq t)$ debe ser 1.


Observación:

\begin{itemize}
\item Si $F$ es una distribución de Poisson de parámetro $\lambda>0$, $M_F$ es infinito. 
\item Si $k$ es un natural, entonces:
\begin{eqnarray}
\frac{P(X>k)}{P(X\geq k)} &=& \frac{P(X \geq k+1)}{P(X\geq k)} \\ \nonumber
&=& 1-\frac{P(X=k)}{P(X \geq k)} \approx 1-\left(1- \frac{\lambda}{k}\right) 
\end{eqnarray}
que tiende a $0$ cuando $k$ tiende a infinito, por lo cual $F$ NO admite DEA, o sea que no se puede aproximar el máximo de una sucesión $iid$ de variables de Poisson.
\end{itemize}

Observación:

El Teorema 6 brinda una condición NECESARIA pero NO SUFICIENTE para DEA. Un ejemplo de ello lo aportó Von Mises, mostrando que la distribución

$$F(x)= 1- e^{(-x-sen(x))}$$
cumple con la condicion del Teorema 6 pero no admite DEA.

### Definición 4: Distribución max-estables

Si dada una $F$ distribución, $X$ con distribución $F$, $k$ natural arbitrario y $X_1,...,X_k$ es $iid$ con distribución $F$, existen reales $a_k$, $b_k$ tales que $max(X_1,...,X_k)$ tiene la misma distribución que $a_k X+ b_k$, $F$ se dice \textit{max-estable}.

El Teorema FTG resulta de superponer los dos siguientes teoremas:

##### Teorema 7

\begin{itemize}
  \item[a)] Si $F$ admite $DEA\;H$, entonces $H$ es max-estable.
  \item[b)] Si $H$ es max-estable, es la DEA de sí misma.
\end{itemize}

##### Teorema 8

Una distribución es max-estable si y solo si es extremal\footnote{O sea Gumbel, Weibull o Fréchet}.
El Teorema 7 es bastante intuitivo y análogo a los teoremas de Lévy sobre distribuciones estables en aproximaciones asintóticas de las distribuciones de sumas. Para el Teorema 8 haremos enseguida un ejercicio sencillo que nos ayudará a hacerlo creíble.
Luego precisaremos, para terminar con esta parte, cómo son las distribuciones que tienen por DEA cada uno de los tres tipos de distribuciones extremales. Para eso precisamos recordar algunas definiciones, como la siguiente.


Obsrvación:

Si $F$ y $G$ son dos distribuciones, tienen colas equivalentes si $M_F=M_G$ y cuando $t$ tiende a $M_F$ por izquierda, $(1-F(t))/(1-G(t))$ tiende a un valor $c>0$.
Recordando ahora cómo se calcula la distribución del máximo de dos variables independientes, es muy sencillo calcular la distribución del $max\{X,Y\}$, cuando $X$ e $Y$ son independientes y cada una de ellas es una distribución extremal. 

Se tiene el siguiente resultado:

| $X$ | $Y$| $max(X,Y)$ |
|-------|-------|--------------|
| \textcolor{red}{Weibull} | \textcolor{red}{Weibull} | \textcolor{red}{Weibull} |
| \textcolor[rgb]{0.0,0.5,0.0}{Weibull} | \textcolor[rgb]{0.0,0.5,0.0}{Gumbel} | \textcolor[rgb]{0.0,0.5,0.0}{Cola equivalente Gumbel} |
| \textcolor{blue}{Weibull} | \textcolor{blue}{Fréchet} | \textcolor{blue}{Fréchet} |
| \textcolor[rgb]{0.0,0.5,0.0}{Gumbel} | \textcolor[rgb]{0.0,0.5,0.0}{Weibull} | \textcolor[rgb]{0.0,0.5,0.0}{Cola equivalente Gumbel} |
| \textcolor{red}{Gumbel} | \textcolor{red}{Gumbel} | \textcolor{red}{Gumbel} |
| \textcolor{blue}{Gumbel} | \textcolor{blue}{Fréchet} | \textcolor{blue}{Cola equivalente Fréchet} |
| \textcolor{blue}{Fréchet} | \textcolor{blue}{Weibull} | \textcolor{blue}{Fréchet} |
| \textcolor{blue}{Fréchet} | \textcolor{blue}{Gumbel} | \textcolor{blue}{Cola equivalente Fréchet} |
| \textcolor{red}{Fréchet} | \textcolor{red}{Fréchet}| \textcolor{red}{Fréchet} |


\textcolor{red}{\rule{1em}{1em} Las extremales son max-estables: tomar máximos de dos del mismo tipo queda en el mismo tipo.}


\textcolor[rgb]{0.0,0.5,0.0}{\rule{1em}{1em} Gumbel es más pesada que Weibull. En la cola, que es lo que cuenta para máximos, prima Gumbel.}


\textcolor{blue}{\rule{1em}{1em} Fréchet es más pesada que Gumbel y mucho más pesada que Weibull.}
\vspace{1cm}

Además, de la tabla se deduce que 

##### Teorema 9 

Si $X_1,...,X_n$ independientes y cada $X_i$ tiene uno de los tres tipos de distribución extremal, entonces la distribución del $max(X_1,...,X_n)$ es:
\begin{itemize}
\item[a)] Cola equivalente a Fréchet, si alguna de las variables es Fréchet y alguna otra es Gumbel.
\item[b)]  Fréchet, si alguna es Fréchet y ninguna es Gumbel.
\item[c)]  Cola equivalente Gumbel ninguna es Fréchet pero algunas son Gumbel y otras Weibull.
\item[d)] Gumbel si todas son Gumbel.
\item[e)]  Weibull si todas son Weibull.
\end{itemize}

Observación:

Si $F$ es una distribución, se dice que tiene \textit{cola de variación regular de orden} $-\alpha$, para $\alpha \geq 0$, si para todo $t>0$, $(1-F(tx))/(1-F(x))$ tiende
a $t^{-\alpha}$ si $x \rightarrow  \infty$. Para abreviar se dirá que $F$ es $R_{-\alpha}$. Por ejemplo, para $\alpha=3$, un caso de una tal $F$ es $F(u)=1- 1/u^3$.


Por otra parte se dice que $L$ es una \textit{función de variación lenta} si, para todo $t>0$, $L(tx)/L(x)$ tiende a 1 cuando $x \rightarrow  \infty$. Por ejemplo, $L(u)=log(u)$.

\newpage

### Definición 4: Dominio de atracción maximal

Si $H$ es una distribución extremal (Gumbel, Weibull o Fréchet) su Dominio de Atracción Maximal ($DAM(H)$) está constituído por todas las distribuciones $F$ que tienen $DEA\;H$.

##### Teorema 9: DAM de la Fréchet

$F$ pertenece a la DAM de $\Phi_{\alpha}$ si y sólo si
$1-F(x)=x-\alpha L(x)$ para alguna $L$ de variación lenta,
lo cual es equivalente a decir que $F$ es $R_{-\alpha}$.


##### Corolario 1: DAM de la Fréchet
Si $F$ es una distribución con densidad $f$ que cumple que $xf(x)/(1-F(x))$ tiende a $\alpha$ cuando $x \rightarrow  \infty$
se dice que $F$ cumple la Condición de Von Mises I. En tal caso, $F$ pertenece a la DAM de $\Phi_{\alpha}$ y mas aún, la DAM de $\Phi_{\alpha}$ son todas las distribuciones que tienen cola equivalente a alguna distribución que cumpla la Condición de Von Mises I.
Del DAM Fréchet y Teorema 1, surge lo siguiente.


##### Teorema 10: DAM de la Weibull
\begin{itemize}
\item [a)] $F$ pertenece a la DAM de $\Psi_{\alpha}$ si y solo si $M_F$ es finito y además $$1-F(M_F -1/x)=x^{-\alpha} L(x)$$ para alguna
$L$ de variación lenta, es decir que pertenece a $R_{-\alpha}$. Observar que con el cambio de variable $u=M_F -1/x$,
resulta que $1-F(u)=(^{-}MF -u)^{\alpha} L(1/(M_F -u))$ para alguna $L$ de variación lenta, para $u< M_F$. Además puede tomarse $d_n= M_F$ y $c_n= n-\alpha$.
\item [b)] Si $F$ distribución con densidad $f$ positiva en $(a,M_F)$ para algun $a< M_F$ y $(M_F -x)f(x)/(1-F(x))$ tiende a $\alpha$ cuando $x\rightarrow M_F$, se dice que $F$ cumple la Condición de Von Mises II. En tal caso $F$ pertenece a la DAM de $\Psi_{\alpha}$ y mas aún, la DAM de $\Psi_{\alpha}$ son todas las distribuciones que tienen cola equivalente a alguna distribución que cumpla la Condición de Von Mises II.
\end{itemize}

##### Teorema 11: DAM de la Gumbel


Una distribución $F$ se dice una Función de Von Mises con función auxiliar $h$ si existe $a < M_F$ ($M_F$ puede ser finito o infinito) tal que para algún $c>0$ se tiene

$$
1-F(x)= c\;exp^{{- \int_a^X \frac{1}{h(t)} dt}},
$$


con $h$ positiva, con
densidad $h^\prime$ y $h^\prime(x)$ tendiendo a $0$ para $x\rightarrow M_F$
Se tiene entonces que la $DAM$ de $\Lambda$ son todas las distribuciones que tienen cola equivalente a alguna distribución que sea una Función de Von Mises. Básicamente, se trata de colas más livianas que cualquier expresión del tipo $1/x^k$, más aún, con decaimiento \textit{del tipo exponencial}, en el sentido preciso siguiente: si como en el Teorema 11

$1-F(x)= c\;exp^{{- \int_a^X \frac{1}{h(t)} dt}}$, entonces se tiene
$1-F(x)= c\;exp^{-(x-a)/h(x)}$, donde la función auxiliar $h$ es no-decreciente y con asíntota horizontal.

Además, $d_n$ y $c_n$ suelen involucrar expresiones logarítmicas. Más concretamente, $dn = F^{-1}(1-1/n)$,
$c_n = h(d_n)$, donde $F^{-1}$ es la inversa generalizada (o función cuantil), definida por
$F^{-1}(p)= inf\{t / F(t)\geq p\}$, para $0<p<1$.

### Corolario 2 : 

Si $F$ pertenece al $DAM$ Gumbel, $M_F$ es infinito, y se considera $X$ con distribucion $F$, entonces $E(X+k)$ es finito para todo $k$ natural.
Los resultados antes vistos nos permiten reconocer que distribuciones tienen $DEA$ y si la tienen, cual es. Cierran el tema. Adicionalmente, permiten ver con mucha precision que el quid de esta teoría es el comportamiento de las colas de las distribuciones, que Fréchet corresponde a las colas más pesadas, luego la Gumbel y finalmente Weibull.
Para terminar el capítulo presentaremos la distribución de valores extremos generalizada\footnote{GEV, por sus siglas en inglés.}, que es una forma de compactar en una unica fórmula las tres distribuciones extremales, debida a Jenkinson-Von Mises.

### Definición 5: GEV

Se define a la distribución de valores extremos generalizada $(GEV)$\footnote{Por sus siglas en inglés relativas a Generalized Extreme Values.} de posición $\mu$, escala $\beta$ e índice $\xi$ con

$$
G(\mu,\beta,\xi) =
\begin{cases}
    e^{-(1+ \xi(t-\mu)/ \beta)(-1/ \xi)} & \text{si  } \xi \neq 0, \forall\;t\;\text{donde } 1+ \xi(t-\mu)/ \beta) >0 \\
    e^{-e^{(-(t-\mu)/ \beta)}} & \text{si  } \xi =0,\; \forall \;t \\
\end{cases}
$$
\vspace{0.5cm}


En los casos en que $\xi$ tome los siguientes valores, se tiene

\begin{align*}
 \xi=0,  & \text{ corresponde a Gumbel,} \\
 \xi< 0, &\text{ corresponde a Weibull y } \alpha=-1/ \xi \\
 \xi>0, &\text{ corresponde Fréchet y }  \alpha=1/ \xi
\end{align*}

En $R$ existen rutinas para estimar $\xi$ con intervalos de confianza( por máxima verosimilitud, etc.) lo cual da formas de testear si una extremal es Gumbel, Weibull o Fréchet.



Observación: 

En algunas situaciones datos extremales pueden ajustarse a más de un modelo. Por ejemplo, puede ocurrir que tanto ajusten los datos una Gumbel como una Weibull. Frente a estas situaciones, no hay una receta única de cómo proceder sino que quien está modelando debe tener claro si corresponde volcarse hacia cálculos más pesimistas (que dan mayor probabilidad a eventos extremos muy severos) o más optimistas.

Usualmente la opción pesimista implica privilegiar la seguridad y la optimista la economía de recursos, pero insistimos en que la reflexión ante cada caso es indispensable. Un poquito más adelante veremos, al comparar un modelo Gumbel con un modelo Fréchet, que las diferencias pueden ser sumamente drásticas.




Observación:

Antes de seguir adelante, demos la respuesta a la parte $a)$ del Ejercicio 5. Es un ejercicio de Cálculo Diferencial sencillo mostrar que la cola de un $N(0,1)$, es decir $Q(t)=P(X>t)$, donde $X$ tiene distribución $N(0,1)$, es equivalente, para $t$ tendiendo a infinito, a la función $\phi(t)/t$, donde $\phi$ representa la densidad normal típica (campana de Gauss). Basándose en esto, si se considera ahora una variable log-normal $Y$, tal que $log(Y)$ es una $N(0,1)$, puede probarse que su cola $R(t)=P(Y>t)$, es equivalente, para $t$ tendiendo a infinito, a la función $\phi(log(t))/log(t)$. Con un poco más de Cálculo, esta última función puede escribirse para $a>e$ (por ejemplo $a=3$), como


\begin{equation}
c\times e^{-\int_{a}^{t}1/h(s)\; ds} \quad \text{para }\: t>a
\end{equation}


donde $c$ se expresa en función de $a$ y $h(s)=\frac{s\; log(s)}{(log(s))^2+1}$ la cual cumple las hipótesis del Teorema 11.

Se concluye entonces que la log-normal está en el $DAM$ Gumbel, o lo que es lo mismo, que la log- normal admite $DEA$ Gumbel.




Observación: Tiempos y Valores de Retorno


En Ingeniería y Ciencias Ambientales, suele pensarse los eventos extremos (por ejemplo: observación por encima de cierto valor muy alto), en términos de tiempos de retorno (tiempo que se espere para que ocurra un evento). Bajo las hipótesis de datos $iid$, el tiempo de retorno $T$ tiene una distribución $Geo(p)$, con $p = P(evento)$, por lo cual el tiempo de retorno medio es $E(T)=1/p$ y pueden hacerse intervalos de confianza para $E(T)$, en la medida que exista información de $P(evento)$, lo cual puede obtenerse a partir de este capítulo o de los siguientes. Cabe observar que muchas veces se utiliza la expresión Tiempo de Retorno (TR) para $E(T)$.


Más precisamente, $TR(u)$, el Tiempo de retorno del valor $u$, es el valor esperado (o la media) del tiempo que se debe esperar para que la variable en estudio supere el valor $u$, es decir que $TR(u) = 1/P(X>u)$, si $X$ es la variable en estudio.

Por otro la lado, en una mirada inversa, el Valor de Retorno a tiempo $t$, $VR(t)$ es el valor de $u$ para el cual $TR(u)=t$, es decir que $TR(VR(t))=t$ (y también $VR(TR(u))=u$, es decir que $TR$ y $VR$ son, como funciones, inversas una de la otra).


Para \textit{bajar un poco a tierra} estos conceptos, vamos a calcularlos y compararlos cuando la variable $X$ es Gumbel y cuando (con los mismos valores de posición $\mu$ y escala $\beta>0$).

Comencemos por la Gumbel, recordemos que $X$ tiene distribución $\Lambda( \mu,\beta)$ si $X= \mu+\beta Y$ , donde $Y$ tiene distribución $\Lambda$.

Dado entonces un valor $\mu>0$ , otro valor $t>0^*$ resulta que

\begin{itemize}
\item $P(X>u)=1-e^{-e{(u-\mu)/ β }}$
\item $TR(u)=1/P(X>u)$
\item $VR(t)= \mu-\beta\: log\{log\{t/(t-1)\}\}$
\end{itemize}

  (ECUACIONES G)\footnote{Cabe observar que si se supone que las observaciones son diarias (o enteras en la unidad que corresponda), los tiempos de retorno TR se redondean a enteros y los valores de $t$ en la última ecuación se toman enteros.}


Sigamos ahora por la Fréchet, recordemos que $X$ tiene distribución $\Phi_{\alpha}^{( \mu,\beta)}$ si $X= \mu+\beta Y$, donde $Y$ tiene distribución $\Phi_{\alpha}$.

Dado entonces un valor $u>0$, otro valor $t$ entero, resulta que

\begin{itemize}
\item $P(X>u)=1-e^{ -\left \{( u- \mu)/\beta\right \}^{-\alpha}}$,
\item $TR(u)=\frac{1}{P(X>u)}$,
\item $VR(t)= \mu+ \beta\left \{log\left \{ \frac{t}{(t-1)}\right \}\right \}-\frac{1}{\alpha}$
\end{itemize}

(ECUACIONES F)


Para visualizar claramente estos resultados, tabularemos 
y graficaremos los mismos usando en ambos casos:

\begin{itemize}
\item $\mu=15$
\item $\beta=10$
\item $\alpha=2.5$
\item $\xi=0.4$ no muy distante del $\xi=0$ de la Gumbel
\end{itemize}


```{r}
# faltan datos, ya los pedí
```

Tanto las tablas como la gráfica muestran que el modelo Fréchet da probabilidades mucho mayores a valores muy elevados (es más “pesimista”, si los valores mayores representan mayores esfuerzos o problemas).

Tratemos de ver ahora los TR para uno y otro modelo. Es claro que, siguiendo la lógica anterior, es más “pesimista” el modelo que de tiempos de retorno menores en valores elevados.

```{r}
#datos
```

Se observa muy claramente que el modelo Fréchet es mucho más “pesimista”. Veamos ahora los VR. Será en este contexto más “pesimista” quien dé mayores VR.

Resulta evidente el mayor “pesimismo”del modelo Fréchet.
Finalmente, para cerrar el punto, veamos que TR y VR son efectivamente inversas.

Por ejemplo, si tomamos el tiempo $t=90$ días, vemos que en Gumbel su $VR$ es $59,942$ muy ligeramente inferior a 60. En la tabla de TR, vemos que para el valor 60, Gumbel da TR= 91, casi igual a t=90. Si con este mismo $t$ vamos al modelo Fréchet, vemos que su VR es 75,537 algo superior a 74. 

En la tabla de TR vemos que para el valor 75 Fréchet da TR= 89, casi igual a t=90.


Es decir que, salvando las ligeras diferencias fruto de que las tablas son discretas y hay redondeos, etc., hemos corroborado que para $t$ días, tenemos que TR $(VR(t))=t$.
Si tomamos ahora el valor 70, vemos que en Gumbel tiene TR=245, un poco por debajo de 270, cuyo $VR=70,966$. En Fréchet 70 tiene $TR=71$, más abajo que 90, que tiene $VR= 75,357$ bastante cercano a 70.
Haciendo la salvedad de lo artesanal y aproximado de mirar una tabla y no calcular en continuo, queda claro que para un valor u tenemos que $VR(TR(u))=u$.

-->
